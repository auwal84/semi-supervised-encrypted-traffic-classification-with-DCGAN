{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " semi_GAN_in keras",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auwal84/semi-supervised-encrypted-traffic-classification-with-DCGAN/blob/master/semi_GAN_in_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoFCaQeM7cRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219b2247-933c-458a-a430-343e921e3c6f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncGJCzricLz5"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6asfkjW78nh"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2HQ8vhNCUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4192e7-515d-46cd-f704-9df9e1504917"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg8PI0QN94oR"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWpK7l4J-z_V"
      },
      "source": [
        "def load_data():\n",
        "    train_data = np.load('TrainPIMset20.npy')\n",
        "    train_labels = np.load('TrainPIMlabels20.npy')\n",
        "    test_data = np.load('TestPIMset20.npy')\n",
        "    test_labels = np.load('testPIMlabels20.npy')\n",
        "    return (train_data,train_labels),(test_data,test_labels)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iZaH7EAyHTu"
      },
      "source": [
        "(train_data,train_labels),(test_data,test_labels) = load_data()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYerv8I6nQrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89df88ac-8b66-4f81-be51-6d3784922edd"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 20, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-RKk3sRnQMw"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvxqIhKDnrxe"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QviPNvzFnskY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFqZIj7G-6XS"
      },
      "source": [
        "train_data = tf.cast(train_data,tf.float32)\n",
        "train_data = tf.reshape(train_data,[-1,20,3,1])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkPBIkpQyo7T"
      },
      "source": [
        "train_data = tf.cast(train_data,tf.float32)\n",
        "train_data = tf.reshape(train_data,[-1,20,3,1])\n",
        "\n",
        "train_labels = tf.cast(train_labels,tf.int32)\n",
        "train_labels = tf.one_hot(train_labels,4)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fSUoj_0oAyE"
      },
      "source": [
        "test_labels = tf.cast(test_labels,tf.int32)\n",
        "test_labels = tf.one_hot(test_labels,4)\n",
        "test_data = tf.cast(test_data,tf.float32)\n",
        "test_data = tf.reshape(test_data,[-1,20,3,1])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0irHtefSXWQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SrT5hVOoc6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d924eb-7f6e-4e2a-c15d-608d1d75f425"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpdI3SDPEPog"
      },
      "source": [
        "num_classes = 4\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "latent_dim =1920\n",
        "INPUT_SHAPE = (20,3,1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYe7OD_nEWxS"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_data,train_labels))\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE,drop_remainder = True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_data,test_labels)).batch(BATCH_SIZE,  drop_remainder = True)                                                  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAhcNz0aJAWg"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fcdmq0gEarl"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(60*1*16, use_bias=False, input_shape=(latent_dim,)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU(alpha = 1))\n",
        "      \n",
        "    model.add(tf.keras.layers.Reshape((20, 3, 16)))\n",
        "    assert model.output_shape == (None, 20, 3, 16) # Note: None is the batch size\n",
        "    \n",
        "    model.add(tf.keras.layers.Conv2DTranspose(8, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None,20, 3, 8)  \n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(4, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None,20, 3, 4)    \n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None,20, 3, 1)\n",
        "  \n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzviHl1BHkZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7dbb0c-6914-410a-ffe8-1cc59237eb9c"
      },
      "source": [
        "g_model =make_generator_model()\n",
        "g_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 960)               1843200   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 960)               3840      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 20, 3, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 20, 3, 8)          3200      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 20, 3, 8)          32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 20, 3, 8)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 20, 3, 4)          800       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 20, 3, 4)          16        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 20, 3, 4)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 20, 3, 1)          100       \n",
            "=================================================================\n",
            "Total params: 1,851,188\n",
            "Trainable params: 1,849,244\n",
            "Non-trainable params: 1,944\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quCAohswGUwl"
      },
      "source": [
        "\n",
        "def make_discriminator_model():\n",
        "    input = keras.Input(shape=(INPUT_SHAPE))\n",
        "    x= keras.layers.Dropout(0.4)(input)\n",
        "    x= keras.layers.Conv2D(32,kernel_size=(5,5),strides = (1,1),padding = 'same')(x)\n",
        "    x = keras.layers.LeakyReLU()(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x= keras.layers.Conv2D(64,kernel_size =(3,3),strides =(1,1),padding='same')(x)\n",
        "    x= keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.LeakyReLU(0.2)(x)\n",
        "    x = keras.layers.Conv2D(128,kernel_size=(2,2),strides = (1,1),padding='same')(x)\n",
        "    x = keras.layers.LeakyReLU(0.2)(x)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    #x = keras.layers.Dense(4)(x)\n",
        "    model = Model(input,x,name='discriminator')\n",
        "    return model\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fmVA0mB6SP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e5cb4d-3de1-4811-82e5-3433c0868212"
      },
      "source": [
        "d_model = make_discriminator_model()\n",
        "d_model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 20, 3, 1)]        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 20, 3, 1)          0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 20, 3, 32)         832       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 20, 3, 32)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20, 3, 32)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 20, 3, 64)         18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 20, 3, 64)         256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 20, 3, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 20, 3, 128)        32896     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 20, 3, 128)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "=================================================================\n",
            "Total params: 52,480\n",
            "Trainable params: 52,352\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giftczDzbmJu"
      },
      "source": [
        "\n",
        "dense = keras.layers.Dense(5)\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'train_accuracy')\n",
        "precision = tf.keras.metrics.Precision()\n",
        "recall = tf.keras.metrics.Recall()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fodi7xmGwTA"
      },
      "source": [
        "#discriminator loss\n",
        "def loss_values(d_real_features,fake_features,labels,label_rate):\n",
        "    \n",
        "    epsilon = 1e-8\n",
        "    real_logits = dense(d_real_features)\n",
        "    real_prob = tf.nn.softmax( real_logits)\n",
        "    fake_logits =dense(fake_features)\n",
        "    fake_prob = tf.nn.softmax( fake_logits)\n",
        "    def d_loss_fn():\n",
        "      \n",
        "      tmp = tf.nn.softmax_cross_entropy_with_logits(logits = real_logits,\n",
        "                                                  labels = labels)\n",
        "      labeled_mask = np.zeros([BATCH_SIZE], dtype = np.float32)\n",
        "      labeled_count = np.int(BATCH_SIZE * label_rate) # to determine the number of unlabeled data\n",
        "      labeled_mask[range(labeled_count)] = 1.0\n",
        "      D_L_supervised = tf.reduce_sum(labeled_mask * tmp) / tf.reduce_sum(labeled_mask)\n",
        "      #unsupervised loss\n",
        "     \n",
        "      prob_real_be_real = 1 - real_prob[:, -1] + epsilon\n",
        "      tmp_log = tf.math.log(prob_real_be_real)\n",
        "      D_L_unsupervised1 = -1 * tf.reduce_mean(tmp_log)\n",
        "      # data is fake\n",
        "      #d_fake_prob = tf.nn.softmax(fake_logits)\n",
        "      prob_fake_be_fake = fake_prob[:, -1] + epsilon\n",
        "      tmp_log = tf.math.log(prob_fake_be_fake)\n",
        "      D_L_unsupervised2 = -1 * tf.reduce_mean(tmp_log)\n",
        "      \n",
        "\n",
        "      disc_loss = D_L_supervised + D_L_unsupervised1 + D_L_unsupervised2\n",
        "      return disc_loss\n",
        "    def g_loss_fn():\n",
        "     #prob_fake_be_real = 1 - fake_prob[:, -1] + epsilon\n",
        "     #tmp_log =  tf.math.log(prob_fake_be_real)\n",
        "     #G_L1 = -1 * tf.reduce_mean(tmp_log)\n",
        "\n",
        "     real_moments = tf.reduce_mean(d_real_features, axis = 0)\n",
        "     generated_moments = tf.reduce_mean(fake_features, axis = 0)\n",
        "     G_L2 = tf.reduce_mean(tf.abs(real_moments - generated_moments))\n",
        "     #gen_loss = G_L1 +G_L2\n",
        "     return G_L2\n",
        "  \n",
        "    train_accuracy.update_state(labels,real_prob )\n",
        "    precision.update_state(labels,real_prob)\n",
        "    recall.update_state(labels,real_prob)\n",
        "    d_loss = d_loss_fn()\n",
        "    g_loss = g_loss_fn()\n",
        "    return d_loss,g_loss,train_accuracy.result(),precision.result(),recall.result()\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnXI85kbHebk"
      },
      "source": [
        "class semi_gan(Model):\n",
        "  def __init__(self,discriminator,generator, latent_dim,label_rate):\n",
        "    super(semi_gan,self).__init__()\n",
        "    self.discriminator = discriminator\n",
        "    self.generator = generator\n",
        "    self.latent_dim = latent_dim\n",
        "    self.label_rate = label_rate\n",
        "  def compile(self,d_optimizer,g_optimizer,loss_fn):\n",
        "    super(semi_gan,self).compile()\n",
        "    self.d_optimizer = d_optimizer\n",
        "    self.g_optimizer = g_optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    \n",
        "  def extended_labels(self,labels):\n",
        "    extended_label = tf.concat([labels, tf.zeros([tf.shape(labels)[0], 1])], axis = 1)\n",
        "\n",
        "    return extended_label\n",
        "\n",
        "  def train_step(self,dataset):\n",
        "    features = dataset[0]\n",
        "    labels = dataset[1]\n",
        "    latent_vector = tf.random.normal(shape =(BATCH_SIZE, self.latent_dim))\n",
        "    with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
        "      generated_images = self.generator(latent_vector,training = True)\n",
        "      real_features = self.discriminator(features,training=True)\n",
        "      fake_features = self.discriminator(generated_images, training = True)\n",
        "      labels = self.extended_labels(labels)\n",
        "      d_loss,g_loss,train_acc,prec,rec = self.loss_fn(real_features,fake_features,labels,self.label_rate)\n",
        "    d_grad = d_tape.gradient(d_loss,self.discriminator.trainable_variables)\n",
        "    g_grad = g_tape.gradient(g_loss,self.generator.trainable_variables)\n",
        "    self.d_optimizer.apply_gradients(zip(d_grad,self.discriminator.trainable_variables))\n",
        "    self.g_optimizer.apply_gradients(zip(g_grad,self.generator.trainable_variables))\n",
        "    \n",
        "    return {\"d_loss\": d_loss, \"g_loss\": g_loss,\"train_accuracy\":train_acc,\"precision\":prec,\"recall\":rec}\n",
        "  \n",
        "  #evaluate step\n",
        "  def test_step(self,dataset):\n",
        "    features = dataset[0]\n",
        "    labels = dataset[1]\n",
        "    latent_vector = tf.random.normal(shape =(BATCH_SIZE, self.latent_dim))\n",
        "    \n",
        "    generated_images = self.generator(latent_vector,training = False)\n",
        "    real_features = self.discriminator(features,training=False)\n",
        "    fake_features = self.discriminator(generated_images, training = False)\n",
        "    labels = self.extended_labels(labels)\n",
        "    d_loss,g_loss,acc,prec,rec = self.loss_fn(real_features,fake_features,labels,self.label_rate)\n",
        "    return {\"d_loss\": d_loss, \"g_loss\": g_loss,\"accuracy\":acc,\"precision\":prec,\"recall\":rec}\n",
        "\n",
        "      \n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Z-E4MlEKvm"
      },
      "source": [
        "disc_optimizer = keras.optimizers.Adam(1e-4)\n",
        "gen_optimizer = keras.optimizers.Adam(1e-4)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzeMGKFoHn8Y"
      },
      "source": [
        "gan =semi_gan(discriminator=d_model,generator=g_model,latent_dim=latent_dim,label_rate =0.8) "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShKjY5yNP9g_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiJEM06biOEh"
      },
      "source": [
        "gan.compile(d_optimizer=disc_optimizer,\n",
        "            g_optimizer= gen_optimizer,loss_fn= loss_values)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BtUfXl7SzbE"
      },
      "source": [
        "#create a call backs\n",
        "class GAN_monitor(keras.callbacks.Callback):\n",
        "  def __init__(self,validation_data):\n",
        "    super(GAN_monitor,self).__init__()\n",
        "    self.dataset = validation_data\n",
        "  def on_each_end(self,epoch, logs = None):\n",
        "    features = self.dataset[0]\n",
        "    labels = self.dataset[1]\n",
        "    labels = tf.concat([labels, tf.zeros([tf.shape(labels)[0], 1])], axis = 1)\n",
        "    t_real_features = self.model.discriminator(features)\n",
        "    t_real_logits = dense(t_real_features)\n",
        "    t_real_prob = tf.nn.softmax(t_real_logits)\n",
        "    acc = accuracy(labels,t_real_prob)\n",
        "    prec = precision(labels,t_real_prob)\n",
        "    rec = recall(labels,t_real_prob)\n",
        "    print('epoch: %d, | validation_acc: %f,precision: %f,recall:%f' %(epoch,accuracy.result().numpy,precision.result().numpy(),recall.result().numpy()))\n",
        "    \n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaBe1tGmDojc"
      },
      "source": [
        "epochs = 10\n",
        "#cbk = GAN_monitor(test_dataset)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ps7UHL2j7xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238f7ec5-03ae-4543-e2fb-107454edb138"
      },
      "source": [
        "\n",
        "history = gan.fit(train_dataset,epochs=epochs,validation_data = test_dataset)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "234/234 [==============================] - 4s 15ms/step - d_loss: 2.7843 - g_loss: 0.0314 - train_accuracy: 0.4809 - precision: 0.5722 - recall: 0.1561 - val_d_loss: 2.3333 - val_g_loss: 0.0181 - val_accuracy: 0.4762 - val_precision: 0.7072 - val_recall: 0.2213\n",
            "Epoch 2/10\n",
            "234/234 [==============================] - 3s 13ms/step - d_loss: 3.2576 - g_loss: 0.0195 - train_accuracy: 0.4090 - precision: 0.6101 - recall: 0.1863 - val_d_loss: 2.2376 - val_g_loss: 0.0155 - val_accuracy: 0.4153 - val_precision: 0.6444 - val_recall: 0.2064\n",
            "Epoch 3/10\n",
            "234/234 [==============================] - 3s 13ms/step - d_loss: 2.9671 - g_loss: 0.0297 - train_accuracy: 0.3933 - precision: 0.6376 - recall: 0.1842 - val_d_loss: 2.1565 - val_g_loss: 0.0375 - val_accuracy: 0.4048 - val_precision: 0.6380 - val_recall: 0.1975\n",
            "Epoch 4/10\n",
            "234/234 [==============================] - 3s 13ms/step - d_loss: 2.8482 - g_loss: 0.0406 - train_accuracy: 0.3969 - precision: 0.6357 - recall: 0.1867 - val_d_loss: 1.9425 - val_g_loss: 0.0228 - val_accuracy: 0.4080 - val_precision: 0.6404 - val_recall: 0.1996\n",
            "Epoch 5/10\n",
            "234/234 [==============================] - 3s 13ms/step - d_loss: 2.6512 - g_loss: 0.0520 - train_accuracy: 0.4056 - precision: 0.6411 - recall: 0.1929 - val_d_loss: 1.7518 - val_g_loss: 0.0333 - val_accuracy: 0.4171 - val_precision: 0.6465 - val_recall: 0.2080\n",
            "Epoch 6/10\n",
            "234/234 [==============================] - 3s 14ms/step - d_loss: 2.4526 - g_loss: 0.0679 - train_accuracy: 0.4179 - precision: 0.6473 - recall: 0.2053 - val_d_loss: 1.6101 - val_g_loss: 0.0414 - val_accuracy: 0.4293 - val_precision: 0.6519 - val_recall: 0.2229\n",
            "Epoch 7/10\n",
            "234/234 [==============================] - 3s 14ms/step - d_loss: 2.2617 - g_loss: 0.0838 - train_accuracy: 0.4316 - precision: 0.6532 - recall: 0.2230 - val_d_loss: 1.4723 - val_g_loss: 0.0533 - val_accuracy: 0.4424 - val_precision: 0.6575 - val_recall: 0.2406\n",
            "Epoch 8/10\n",
            "234/234 [==============================] - 3s 14ms/step - d_loss: 2.0773 - g_loss: 0.0997 - train_accuracy: 0.4457 - precision: 0.6601 - recall: 0.2425 - val_d_loss: 1.3688 - val_g_loss: 0.0666 - val_accuracy: 0.4555 - val_precision: 0.6634 - val_recall: 0.2589\n",
            "Epoch 9/10\n",
            "234/234 [==============================] - 3s 14ms/step - d_loss: 1.8862 - g_loss: 0.1177 - train_accuracy: 0.4599 - precision: 0.6683 - recall: 0.2622 - val_d_loss: 1.3004 - val_g_loss: 0.0789 - val_accuracy: 0.4688 - val_precision: 0.6707 - val_recall: 0.2772\n",
            "Epoch 10/10\n",
            "234/234 [==============================] - 3s 14ms/step - d_loss: 1.6975 - g_loss: 0.1392 - train_accuracy: 0.4740 - precision: 0.6767 - recall: 0.2818 - val_d_loss: 1.2516 - val_g_loss: 0.0929 - val_accuracy: 0.4818 - val_precision: 0.6782 - val_recall: 0.2955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1spmJsJkaZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ed079c-a860-435e-d824-490667c029ae"
      },
      "source": [
        "gan.evaluate(test_dataset)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 0s 5ms/step - d_loss: 4.1802 - g_loss: 0.0899 - accuracy: 0.4787 - precision: 0.6681 - recall: 0.2938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cuZz_B_waRh"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxPKf12gAoN0"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}